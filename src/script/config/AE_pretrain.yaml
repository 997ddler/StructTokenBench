defaults:
    - _self_
    - trainer: base
    - lightning: pretrain
    - data: pretrain_pdb
    - optimization: base
    - autorestart: base
model:
    class_name: "VQVAEModel"
    pretrained_ckpt_path: "" # where to load the pretrained pLM
    ckpt_path: null
    quantizer: null # null for AE
    encoder:
        d_model: 1024
        n_heads: 1
        v_heads: 128
        n_layers: 2
        d_out: 1024 # original 128
    decoder:
        d_model: 1024
        n_heads: 16
        n_layers: 8
deepspeed_path: src/script/config/deepspeed/32_stage2.json

default_data_dir: null
tokenizer: null
max_steps: 10000
experiment_name: test # used for tensorboard
run_name: test_run
validate_only: false
test_only: false

save_dir_path: "" # would be indicated during running
tokenizer_device: "cuda" # or "cpu": where to put the tokenizer.tokenizer_encoder. 

checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    every_n_train_steps: null
    every_n_epochs: 10  # Keep saving every epoch for "last"
    save_top_k: 1  # Or keep your current value
    monitor: "validation_loss"  # Adjust based on your metrics
    mode: "min"
    filename: "last-{epoch}-{step}"
    save_last: True  # This saves the last checkpoint
    dirpath: null

# Add a new periodic checkpoint callback
periodic_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    every_n_train_steps: null
    every_n_epochs: 10  # Save every 10 epochs
    save_top_k: -1  # Keep all periodic checkpoints
    monitor: null  # Don't monitor, just save periodically
    filename: "epoch-{epoch:03d}-{step}"
    save_last: False
    dirpath: null
